{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_clf_problem(X, y, h=None):\n",
    "    '''\n",
    "    Plots a two-dimensional labeled dataset (X,y) and, if function h(x) is given, \n",
    "    the decision surfaces.\n",
    "    '''\n",
    "    assert X.shape[1] == 2, \"Dataset is not two-dimensional\"\n",
    "    if h!=None : \n",
    "        # Create a mesh to plot in\n",
    "        r = 0.03  # mesh resolution\n",
    "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, r),\n",
    "                             np.arange(y_min, y_max, r))\n",
    "        XX=np.c_[xx.ravel(), yy.ravel()]\n",
    "        try:\n",
    "            Z_test = h(XX)\n",
    "            if Z_test.shape == ():\n",
    "                # h returns a scalar when applied to a matrix; map explicitly\n",
    "                Z = np.array(list(map(h,XX)))\n",
    "            else :\n",
    "                Z = Z_test\n",
    "        except ValueError:\n",
    "            # can't apply to a matrix; map explicitly\n",
    "            Z = np.array(list(map(h,XX)))\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        plt.contourf(xx, yy, Z, cmap=plt.cm.Pastel1)\n",
    "\n",
    "    # Plot the dataset\n",
    "    plt.scatter(X[:,0],X[:,1], c=y, cmap=plt.cm.tab20b, marker='o', s=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression metrics\n",
    "\n",
    "$$\\text{Mean Squared Error} = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i)^2$$\n",
    "\n",
    "$$\\text{Mean Absolute Error} = \\frac{1}{N} \\sum_{i=1}^{N} \\left | \\hat{y}_i - y_i \\right |$$\n",
    "\n",
    "$$\\text{Root Mean Squared Error} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i)^2}$$\n",
    "\n",
    "$$R^2 = 1 - \\frac{\\sum_{i=1}^{N} (\\hat{y}_i - y_i)^2}{\\sum_{i=1}^{N} (y_i - \\bar{y})^2} = 1 - \\frac{\\text{MSE}}{\\sigma^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mean_squared_error(y_test, y_pred):\n",
    "    return np.mean((y_test - y_pred)**2)\n",
    "\n",
    "def _mean_absolute_error(y_test, y_pred):\n",
    "    return np.mean(np.abs(y_test - y_pred))\n",
    "\n",
    "def _root_mean_squared_error(y_test, y_pred):\n",
    "    return np.sqrt(_mean_squared_error(y_test, y_pred))\n",
    "\n",
    "def _r2_score(y_test, y_pred):\n",
    "    return 1 - _mean_squared_error(y_test, y_pred) / np.var(y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples=90, n_features=1, n_informative=2, noise=5)\n",
    "plt.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "plt.scatter(X, y)\n",
    "x = np.linspace(-3, 3, 100)\n",
    "plt.plot(x, model.predict(x.reshape(-1, 1)), color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (mean_absolute_error, mean_squared_error, r2_score)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean absolute error: {mae}')\n",
    "print(f'Mean squared error: {mse}')\n",
    "print(f'Root mean squared error: {np.sqrt(mse)}')\n",
    "print(f'R2 score: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = _mean_absolute_error(y_test, y_pred)\n",
    "mse = _mean_squared_error(y_test, y_pred)\n",
    "rmse = _root_mean_squared_error(y_test, y_pred)\n",
    "r2 = _r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean absolute error: {mae}')\n",
    "print(f'Mean squared error: {mse}')\n",
    "print(f'Root mean squared error: {rmse}')\n",
    "print(f'R2 score: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Classification metrics\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{1}{N} \\sum_{i=1}^{N} 1\\{\\hat{y}_i = y_i\\}$$\n",
    "\n",
    "$$\n",
    "\\text{Confusion Matrix} =\n",
    "\n",
    "\\left(\n",
    "\\begin{matrix}\n",
    "  \\text{TN} & \\text{FP} \\\\\n",
    "  \\text{FN} & \\text{TP}\n",
    "\\end{matrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Precision} &= \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}, & 0 < P < 1 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Recall} &= \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}, & 0 < R < 1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\\text{F1 Score} = \\frac{2}{ \\frac{1}{\\text{Precision}} + \\frac{1}{\\text{Recall}} } = \\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _accuracy_score(y_test, y_pred):\n",
    "    return np.mean(y_test==y_pred)\n",
    "\n",
    "def _confusion_matrix(y_test, y_pred):\n",
    "    tp, fn, fp, tn = 0, 0, 0, 0\n",
    "\n",
    "    for y_t, y_p in zip(y_test, y_pred):\n",
    "\n",
    "        if y_t == 1 and y_p == 1:\n",
    "            tp += 1\n",
    "\n",
    "        elif y_t == 1 and y_p == 0:\n",
    "            fp += 1\n",
    "\n",
    "        elif y_t == 0 and y_p == 1:\n",
    "            fn += 1\n",
    "\n",
    "        elif y_t == 0 and y_p == 0:\n",
    "            tn += 1\n",
    "\n",
    "    return np.array([[tn, fn],\n",
    "                     [fp, tp]])\n",
    "\n",
    "def _precision_score(y_test, y_pred):\n",
    "    tp, fp = 0, 0\n",
    "\n",
    "    for y_t, y_p in zip(y_test, y_pred):\n",
    "\n",
    "        if y_t == 1 and y_p == 1:\n",
    "            tp += 1\n",
    "\n",
    "        elif y_t == 0 and y_p == 1:\n",
    "            fp += 1\n",
    "\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "def _recall_score(y_test, y_pred):\n",
    "    tp, fn = 0, 0\n",
    "\n",
    "    for y_t, y_p in zip(y_test, y_pred):\n",
    "\n",
    "        if y_t == 1 and y_p == 1:\n",
    "            tp += 1\n",
    "\n",
    "        elif y_t == 1 and y_p == 0:\n",
    "            fp += 1\n",
    "\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "def _f1_score(y_test, y_pred):\n",
    "\n",
    "    precision = _precision_score(y_test, y_pred)\n",
    "    recall = _recall_score(y_test, y_pred)\n",
    "\n",
    "    return 2 * precision * recall / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=23, n_features=2, n_informative=2, n_redundant=0)\n",
    "plot_2d_clf_problem(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "\n",
    "model = AdaBoostClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "plot_2d_clf_problem(X, y, model.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (accuracy_score, confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score)\n",
    "\n",
    "print(f'Accuracy score: {accuracy_score(y_test, y_pred)}')\n",
    "print(f'Confusion matrix: \\n{confusion_matrix(y_test, y_pred)}')\n",
    "print(f'Precision score: {precision_score(y_test, y_pred)}')\n",
    "print(f'Recall score: {recall_score(y_test, y_pred)}')\n",
    "print(f'F1 score: {f1_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy score: {_accuracy_score(y_test, y_pred)}')\n",
    "print(f'Confusion matrix: \\n{_confusion_matrix(y_test, y_pred)}')\n",
    "print(f'Precision score: {_precision_score(y_test, y_pred)}')\n",
    "print(f'Recall score: {_recall_score(y_test, y_pred)}')\n",
    "print(f'F1 score: {_f1_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
